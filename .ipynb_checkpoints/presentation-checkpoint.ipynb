{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bypassing LLM Safeguards With Homoglyphs\n",
    "\n",
    "## Aldan Creo\n",
    "\n",
    "<img src=\"imgs/qr_repo.png\" width=\"300\"/>\n",
    "\n",
    "https://github.com/ACMCMC/homoglyphs-workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "\n",
    "In this workshop, we will explore how to circumvent mechanisms that protect large language models (LLMs) from malicious instructions using homoglyphs. As LLMs become increasingly capable of generating realistic content, the need for robust protection and detection methods increases. Homoglyphs are characters that appear visually identical but have different Unicode encodings. We will demonstrate how these characters can be used to manipulate the tokenization process, allowing for circumvention of standard security mechanisms in LLM systems.\n",
    "\n",
    "During the workshop we will conduct practical demonstrations of:\n",
    "- How homoglyph attacks work against modern LLM detectors\n",
    "- The technical reasons for the vulnerability of these systems\n",
    "- Methods for identifying such attacks\n",
    "- Strategies for improving defenses against these techniques\n",
    "\n",
    "Who this is for: Cybersecurity professionals, AI researchers, and programmers with an interest in the security of AI systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some notes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a deep dive, will be a bit technical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Will use code, you can download the repo to follow along - but don't have to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Don't need to know about AI, but can help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "1. Create a virtual environment:\n",
    "    ```\n",
    "    python -m venv env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "2. Activate the virtual environment:\n",
    "    - On Windows:\n",
    "      ```\n",
    "      .\\env\\Scripts\\activate\n",
    "      ```\n",
    "    - On macOS/Linux:\n",
    "      ```\n",
    "      source env/bin/activate\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "3. Install the dependencies:\n",
    "    ```\n",
    "    pip install -r requirements.txt\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Homoglyphs 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "CHAR_1 = \"a\"\n",
    "CHAR_2 = \"–∞\"\n",
    "\n",
    "display(Markdown(f\"# {CHAR_1}\"))\n",
    "display(Markdown(f\"# {CHAR_2}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "print(CHAR_1 == CHAR_2)\n",
    "print(unicodedata.name(CHAR_1))\n",
    "print(CHAR_1.encode().hex())\n",
    "print(unicodedata.name(CHAR_2))\n",
    "print(CHAR_2.encode().hex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does AI see the text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/tokenizer.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"silverspeak/reuter\", split='train')\n",
    "display(dataset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "\n",
    "\n",
    "def initialize_tokenizer():\n",
    "    # Initialize a Byte-Pair Encoding (BPE) tokenizer\n",
    "    dummy_tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Set a normalizer to convert text to lowercase and strip accents\n",
    "    dummy_tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    "    )\n",
    "\n",
    "    # Set a pre-tokenizer to split text into words - this ensures that the tokenizer operates at the word level\n",
    "    dummy_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    return dummy_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the trainer for the BPE tokenizer\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000)\n",
    "\n",
    "# Extract text from the dataset\n",
    "texts = [item[\"text\"] for item in dataset]\n",
    "\n",
    "# Train the tokenizer on the dataset's text\n",
    "dummy_tokenizer_small = initialize_tokenizer()\n",
    "dummy_tokenizer_small.train_from_iterator(texts[:10], trainer)\n",
    "\n",
    "# Train the tokenizer on the dataset's text\n",
    "dummy_tokenizer_full = initialize_tokenizer()\n",
    "dummy_tokenizer_full.train_from_iterator(texts, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# See how the tokenizer behaves before training\n",
    "sentence = \"What's the weather in Sofia? Currently, it's a bit cloudy.\"\n",
    "\n",
    "tokenized_small = dummy_tokenizer_small.encode(sentence)\n",
    "tokenized_full = dummy_tokenizer_full.encode(sentence)\n",
    "\n",
    "# Display the tokens before and after training\n",
    "print(\"Tokens after training with 10 texts:\", tokenized_small.tokens, tokenized_small.ids)\n",
    "print(\n",
    "    \"Tokens after training with all texts:\", tokenized_full.tokens, tokenized_full.ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(list(dummy_tokenizer_small.get_vocab().items()), key= lambda e: e[1], reverse=True)[:10])\n",
    "print(sorted(list(dummy_tokenizer_full.get_vocab().items()), key= lambda e: e[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import silverspeak\n",
    "\n",
    "# See how the tokenizer behaves before training\n",
    "sentence_replaced = silverspeak.random_attack(\n",
    "    sentence, percentage=0.1, types_of_homoglyphs_to_use=[\"identical\"]\n",
    ")\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "tokenized_full_attacked = dummy_tokenizer_full.encode(sentence_replaced)\n",
    "\n",
    "print(\n",
    "    \"Tokens before attack:\", tokenized_full.tokens, tokenized_full.ids\n",
    ")\n",
    "print(\n",
    "    \"Tokens after attack:\", tokenized_full_attacked.tokens, tokenized_full_attacked.ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How LLMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Different types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*YkBdN40yy0C27hlb-vNVpA.png\" width=\"400\"/>\n",
    "\n",
    "> Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Detectors based on a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"imgs/classifier.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Download the model and tokenizer\n",
    "cl_model_name = \"SJTU-CL/RoBERTa-large-ArguGPT\"\n",
    "cl_tokenizer = AutoTokenizer.from_pretrained(cl_model_name)\n",
    "cl_model = AutoModelForSequenceClassification.from_pretrained(cl_model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "# Take some random examples from the dataset\n",
    "sampled_data = dataset.shuffle(seed=42).select(range(50))\n",
    "\n",
    "def tokenize_and_get_embeddings(texts, tokenizer, model, max_length=128):\n",
    "    # Tokenize the text data\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    # Get embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.roberta(\n",
    "            inputs[\"input_ids\"].to(model.device), attention_mask=inputs[\"attention_mask\"].to(model.device)\n",
    "        ).last_hidden_state\n",
    "        embeddings = embeddings.mean(dim=1).cpu()  # Mean pooling to get sentence embeddings\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "embeddings = tokenize_and_get_embeddings(texts=sampled_data[\"text\"], tokenizer=cl_tokenizer, model=cl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What embeddings look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(embeddings[0])\n",
    "print(embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How can we \"see\" them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## üëâ Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![pca](https://www.researchgate.net/profile/Mohammad-Reza-Feizi-Derakhshi/publication/368664623/figure/fig1/AS:11431281121355243@1676949795819/UMAP-projections-of-a-3D-woolly-mammoth-skeleton-10.ppm)\n",
    "\n",
    "> Ranjbar-Khadivi, Mehrdad, et al. \"Persian topic detection based on Human Word association and graph embedding.\" arXiv preprint arXiv:2302.09775 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_pca_embeddings(embeddings, true_labels):\n",
    "    # Apply PCA to reduce dimensions to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings.numpy())\n",
    "\n",
    "    # Plot the reduced embeddings with true labels as colors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in set(true_labels):\n",
    "        indices = [i for i, l in enumerate(true_labels) if l == label]\n",
    "        plt.scatter(\n",
    "            reduced_embeddings[indices, 0],\n",
    "            reduced_embeddings[indices, 1],\n",
    "            label=f\"Label {label}\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(\"PCA of the Samples\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_embeddings(embeddings, true_labels=sampled_data[\"generated\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's replace with homoglyphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import silverspeak\n",
    "\n",
    "replaced_texts = sampled_data.map(\n",
    "    lambda x: {\n",
    "        \"replaced_texts\": silverspeak.random_attack(\n",
    "            x[\"text\"], percentage=0.15, types_of_homoglyphs_to_use=[\"identical\"]\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import difflib\n",
    "\n",
    "def display_text_diff(index):\n",
    "    original_text = sampled_data[index][\"text\"]\n",
    "    replaced_text_example = replaced_texts[index][\"replaced_texts\"]\n",
    "    diff = difflib.HtmlDiff().make_table(\n",
    "        original_text.split()[:10], \n",
    "        replaced_text_example.split()[:10], \n",
    "        fromdesc=\"Original Text\", \n",
    "        todesc=\"Replaced Text\", \n",
    "        context=True, \n",
    "        numlines=2\n",
    "    )\n",
    "    display(HTML(diff))\n",
    "\n",
    "# Example usage\n",
    "display_text_diff(0)  # Replace 0 with the desired index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and get embeddings for replaced texts\n",
    "replaced_embeddings = tokenize_and_get_embeddings(\n",
    "    texts=replaced_texts[\"replaced_texts\"], tokenizer=cl_tokenizer, model=cl_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA and plot embeddings for replaced texts\n",
    "plot_pca_embeddings(replaced_embeddings, true_labels=sampled_data['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA and plot embeddings for both types of texts at the same time (we'll have 4 classes now)\n",
    "combined_embeddings = torch.cat([embeddings, replaced_embeddings], dim=0)\n",
    "combined_labels = [f'O_{lab}' for lab in sampled_data['generated']] + [f'R_{lab}' for lab in sampled_data['generated']]\n",
    "plot_pca_embeddings(combined_embeddings, combined_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detectors based on perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The LLMs we hear about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "gemma = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-pt\", device_map=\"auto\"\n",
    ")\n",
    "gemma_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-3-1b-pt\", padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(sentence)\n",
    "# Tokenize the sentence\n",
    "inputs = gemma_tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "tokens = gemma_tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n",
    "print(list(zip(tokens, inputs['input_ids'].squeeze().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain the probability distributions from the model\n",
    "with torch.no_grad():\n",
    "    logits = gemma(\n",
    "        input_ids=inputs[\"input_ids\"].to(gemma.device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(gemma.device)\n",
    "    ).logits.cpu()\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "print(probabilities[0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the top 10 probabilities and their indices\n",
    "top_probs, top_indices = torch.topk(probabilities[0, -2, :], 10)\n",
    "\n",
    "# Convert indices to tokens\n",
    "top_tokens = gemma_tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
    "\n",
    "# Print the tokens and their probabilities\n",
    "for token, prob in zip(top_tokens, top_probs.tolist()):\n",
    "    print(f\"Token: {token}, Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://media.istockphoto.com/id/525965136/photo/says.jpg?s=612x612&w=0&k=20&c=cmODKtBuYlgpJBncyqSRFH030L2PXLdGhg45UDB4Zx0=\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\text{Entropy} = - \\sum_{i=1}^n p(x_i) \\log p(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Store entropy values for plotting\n",
    "entropy_values = []\n",
    "\n",
    "# Recalculate entropy for each unfair_side_prob\n",
    "for unfair_side_prob in torch.linspace(0.0, 1.0, 10):\n",
    "    unfair_side_prob = unfair_side_prob.item()\n",
    "    fair_side_prob = 1.0 - unfair_side_prob\n",
    "    fair_side_prob /= 5\n",
    "    die = torch.distributions.Categorical(\n",
    "        probs=torch.tensor([unfair_side_prob, fair_side_prob, fair_side_prob, fair_side_prob, fair_side_prob, fair_side_prob])\n",
    "    )\n",
    "    entropy_values.append(die.entropy().item())\n",
    "\n",
    "# Plot the entropy values\n",
    "plt.plot(torch.linspace(0.0, 1.0, 10).numpy(), entropy_values, marker='o')\n",
    "plt.title(\"Entropy vs Unfair Side Probability\")\n",
    "plt.xlabel(\"Unfair Side Probability\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Store entropy values for plotting\n",
    "coin_entropy_values = []\n",
    "\n",
    "# Calculate the entropy values for 100 possible coin heads-tails probabilities (from 0.0 to 1.0)\n",
    "for p_heads in torch.linspace(0.0, 1.0, 100):\n",
    "    p_heads = p_heads.item()\n",
    "    p_tails = 1.0 - p_heads\n",
    "    coin = torch.distributions.Categorical(\n",
    "        probs=torch.tensor([p_heads, p_tails])\n",
    "    )\n",
    "    coin_entropy_values.append(coin.entropy().item())\n",
    "\n",
    "# Plot the entropy values\n",
    "plt.plot(torch.linspace(0.0, 1.0, 100).numpy(), coin_entropy_values, marker=None)\n",
    "plt.title(\"Entropy vs Probability of Heads\")\n",
    "plt.xlabel(\"Probability of Heads\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\text{Cross-Entropy} = - \\frac{1}{n} \\sum_i^n p(pred_i) \\log p(label_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\n",
    "\\text{Log-Perplexity} = - \\frac{1}{n} \\sum_{i=1}^n \\log p(x_i)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\text{Perplexity} = e ^ {\\text{Cross-Entropy}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$\n",
    "\\text{Log-Perplexity} = \\log \\left( \\text{Perplexity} \\right) = \\log \\left( e^{\\text{Cross-Entropy}} \\right) = \\text{Cross-Entropy}\n",
    "$\n",
    "\n",
    "_[Log-Perplexity is equivalent to Cross-Entropy as the logarithmic and exponential functions cancel each other out.]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perplexity with real LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def perplexity(\n",
    "    texts,\n",
    "    model: transformers.PreTrainedModel,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "):\n",
    "    tokenized_texts = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    logits = model(\n",
    "        input_ids=tokenized_texts[\"input_ids\"].to(model.device),\n",
    "        attention_mask=tokenized_texts[\"attention_mask\"].to(model.device),\n",
    "    ).logits.cpu()\n",
    "\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_ids = tokenized_texts[\"input_ids\"][:, 1:].contiguous()\n",
    "    shift_attention_mask = tokenized_texts[\"attention_mask\"][:, 1:].contiguous()\n",
    "    loss_mask = shift_attention_mask & ~(tokenized_texts[\"special_tokens_mask\"][:, 1:].contiguous())\n",
    "\n",
    "    # Compute the cross-entropy loss for each token - be sure to ignore invalid positions where the attention mask is 0 (padding)\n",
    "    per_token_ce = cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_ids.view(-1),\n",
    "        reduction=\"none\",\n",
    "    ).view(shift_ids.size()) * loss_mask.bool()\n",
    "\n",
    "    # Compute the mean perplexity across all valid tokens (where attention mask is 1)\n",
    "    mean_perplexity = torch.exp(per_token_ce.sum(dim=1) / loss_mask.sum(dim=1))\n",
    "\n",
    "    tokenized_texts_shifted = {\n",
    "        \"input_ids\": shift_ids,\n",
    "        \"attention_mask\": shift_attention_mask,\n",
    "    }\n",
    "\n",
    "    return per_token_ce, mean_perplexity, tokenized_texts_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get the perplexity of the first text\n",
    "per_token_ce, mean_perplexity, tokenized_texts = perplexity(\n",
    "    texts=[sentence],\n",
    "    model=gemma,\n",
    "    tokenizer=gemma_tokenizer,\n",
    ")\n",
    "print(\"Per-token CE:\", per_token_ce[tokenized_texts['attention_mask'] == 1])\n",
    "print(\"Mean perplexity:\", mean_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_per_token_ce(per_token_perplexity, tokenized_texts, tokenizer):\n",
    "    # Decode the tokens back to text, ignoring positions where the attention mask is 0\n",
    "    tokens = [\n",
    "        tokenizer.decode([token_id])\n",
    "        for token_id, mask in zip(tokenized_texts[\"input_ids\"][0], tokenized_texts[\"attention_mask\"][0])\n",
    "        if mask == 1\n",
    "    ]\n",
    "\n",
    "    # Flatten the perplexity tensor and filter values where the attention mask is 0\n",
    "    perplexity_values = [\n",
    "        value for value, mask in zip(per_token_perplexity[0].tolist(), tokenized_texts[\"attention_mask\"][0]) if mask == 1\n",
    "    ]\n",
    "\n",
    "    # Plot the perplexity values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(tokens)), perplexity_values, tick_label=tokens)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Cross-Entropy Value of Each Token\")\n",
    "    plt.xlabel(\"Tokens\")\n",
    "    plt.ylabel(\"CE\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_per_token_ce(per_token_ce, tokenized_texts, gemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Compute perplexities for the sampled texts in batches of 16 (so that we don't run out of memory)\n",
    "batch_size = 16\n",
    "sampled_per_token_text_cross_entropies = []\n",
    "sampled_perplexities = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    batch_texts = sampled_data[\"text\"][i : i + batch_size]\n",
    "    per_token_ce, mean_perplexity, _ = perplexity(\n",
    "        texts=batch_texts,\n",
    "        model=gemma,\n",
    "        tokenizer=gemma_tokenizer,\n",
    "    )\n",
    "    sampled_per_token_text_cross_entropies.append(per_token_ce)\n",
    "    sampled_perplexities.append(mean_perplexity)\n",
    "\n",
    "sampled_per_token_text_cross_entropies = torch.cat(sampled_per_token_text_cross_entropies, dim=0)\n",
    "sampled_perplexities = torch.cat(sampled_perplexities, dim=0)\n",
    "\n",
    "# Display the mean perplexities for the sampled data\n",
    "print(\"Perplexities for sampled data:\", sampled_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_histogram(data_to_plot, labels):\n",
    "    # Add a small constant to avoid log(0) issues\n",
    "    data_to_plot = [mp + 1e-10 for mp in data_to_plot]\n",
    "\n",
    "    # Plot the histogram with a logarithmic scale\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(\n",
    "        [np.log10(data_to_plot[i]) for i in range(len(labels)) if labels[i] == 0],\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        label=\"Not Generated\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        [np.log10(data_to_plot[i]) for i in range(len(labels)) if labels[i] == 1],\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        label=\"Generated\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    plt.title(\"Histogram of Means Values\")\n",
    "    plt.xlabel(\"Log10(Mean)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    data_to_plot=[mp.item() for mp in sampled_perplexities],\n",
    "    labels=sampled_data[\"generated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Moving away from raw perplexity: cross-perplexity and Binoculars score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\text{Cross-Perplexity}_{(M_1, M_2)}(text) = - \\frac{1}{n} \\sum_{i=1}^n M_1(text_i) \\log M_2(text_i)$\n",
    "\n",
    "‚ö†Ô∏è They need to have the same tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gemma_it = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-it\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cross_perplexity(\n",
    "    texts,\n",
    "    model_1: transformers.PreTrainedModel,\n",
    "    model_2: transformers.PreTrainedModel,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length=128,\n",
    "):\n",
    "    # Tokenize the text data\n",
    "    tokenized_texts = tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\", return_attention_mask=True, max_length=max_length)\n",
    "\n",
    "    # Get logits from model_1\n",
    "    logits_model_1 = model_1(\n",
    "        input_ids=tokenized_texts[\"input_ids\"].to(model_1.device),\n",
    "        attention_mask=tokenized_texts[\"attention_mask\"].to(model_1.device),\n",
    "    ).logits.cpu()\n",
    "\n",
    "    # Get logits from model_2\n",
    "    logits_model_2 = model_2(\n",
    "        input_ids=tokenized_texts[\"input_ids\"].to(model_2.device),\n",
    "        attention_mask=tokenized_texts[\"attention_mask\"].to(model_2.device),\n",
    "    ).logits.cpu()\n",
    "\n",
    "    # Compute probabilities from logits (normalization)\n",
    "    probabilities_1 = torch.nn.functional.softmax(logits_model_1, dim=-1)\n",
    "\n",
    "    # Compute cross-perplexity as the cross-entropy loss between the logits of model 2 and the probabilities of each token ID according to model 1\n",
    "    ce_loss = torch.nn.functional.cross_entropy(\n",
    "        logits_model_2.view(-1, logits_model_2.size(-1)),\n",
    "        probabilities_1.view(-1, probabilities_1.size(-1)),\n",
    "        reduction=\"none\",\n",
    "    ).view(logits_model_2.size()[:-1])\n",
    "\n",
    "    per_token_models_cross_entropy = ce_loss * tokenized_texts[\"attention_mask\"].bool()\n",
    "    cross_perplexities = per_token_models_cross_entropy.sum(dim=1) / tokenized_texts[\"attention_mask\"].sum(dim=1)\n",
    "\n",
    "    return per_token_models_cross_entropy, cross_perplexities, tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Compute perplexities for the sampled texts in batches of 16 (so that we don't run out of memory)\n",
    "batch_size = 16\n",
    "sampled_per_token_models_cross_entropies = []\n",
    "sampled_cross_perplexities = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    batch_texts = sampled_data[\"text\"][i : i + batch_size]\n",
    "    per_token_models_cross_entropies, cross_perplexities, _ = cross_perplexity(\n",
    "        texts=batch_texts,\n",
    "        model_1=gemma_it,\n",
    "        model_2=gemma,\n",
    "        tokenizer=gemma_tokenizer,\n",
    "    )\n",
    "    sampled_per_token_models_cross_entropies.append(per_token_models_cross_entropies)\n",
    "    sampled_cross_perplexities.append(cross_perplexities)\n",
    "\n",
    "sampled_per_token_models_cross_entropies = torch.cat(sampled_per_token_models_cross_entropies, dim=0)\n",
    "sampled_cross_perplexities = torch.cat(sampled_cross_perplexities, dim=0)\n",
    "\n",
    "# Display the mean perplexities for the sampled data\n",
    "print(\"Mean cross perplexities for sampled data:\", sampled_cross_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    data_to_plot=[mp.item() for mp in sampled_cross_perplexities],\n",
    "    labels=sampled_data[\"generated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bringing both metrics together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Perplexity or cross-perplexity can be improved ‚Äì a ratio of both tends to work better!\n",
    "\n",
    "> Hans, Abhimanyu, et al. \"Spotting llms with binoculars: Zero-shot detection of machine-generated text.\" arXiv preprint arXiv:2401.12070 (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the ratio for each text\n",
    "ratios = sampled_perplexities / sampled_cross_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of ratios with corresponding labels\n",
    "plot_histogram(\n",
    "    data_to_plot=[ratio.item() for ratio in ratios],\n",
    "    labels=sampled_data[\"generated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homoglyph attacks\n",
    "\n",
    "## What happens to perplexity, cross-perplexity and their ratio when we use homoglyphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "per_token_ce, mean_perplexity, tokenized_texts = perplexity(\n",
    "    texts=[sentence_replaced],\n",
    "    model=gemma,\n",
    "    tokenizer=gemma_tokenizer,\n",
    ")\n",
    "plot_per_token_ce(per_token_ce, tokenized_texts, gemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import silverspeak\n",
    "\n",
    "replaced_texts = sampled_data.map(\n",
    "    lambda x: {\n",
    "        \"replaced_texts\": silverspeak.random_attack(\n",
    "            x[\"text\"], percentage=0.15, types_of_homoglyphs_to_use=[\"identical\"]\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Regenerate perplexities for the replaced texts in batches of 16\n",
    "batch_size = 16\n",
    "sampled_per_token_text_cross_entropies = []\n",
    "sampled_perplexities = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(sampled_data), batch_size)):\n",
    "    batch_texts = replaced_texts[\"replaced_texts\"][i : i + batch_size]\n",
    "    per_token_ce, mean_perplexity, _ = perplexity(\n",
    "        texts=batch_texts,\n",
    "        model=gemma,\n",
    "        tokenizer=gemma_tokenizer,\n",
    "    )\n",
    "    sampled_per_token_text_cross_entropies.append(per_token_ce)\n",
    "    sampled_perplexities.append(mean_perplexity)\n",
    "\n",
    "sampled_per_token_text_cross_entropies = torch.cat(sampled_per_token_text_cross_entropies, dim=0)\n",
    "sampled_perplexities = torch.cat(sampled_perplexities, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Regenerate cross-perplexities for the replaced texts in batches of 16\n",
    "batch_size = 16\n",
    "sampled_per_token_models_cross_entropies = []\n",
    "sampled_cross_perplexities = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(replaced_texts), batch_size)):\n",
    "    batch_texts = replaced_texts[\"replaced_texts\"][i : i + batch_size]\n",
    "    per_token_models_cross_entropy, cross_perplexities, _ = cross_perplexity(\n",
    "        texts=batch_texts,\n",
    "        model_1=gemma_it,\n",
    "        model_2=gemma,\n",
    "        tokenizer=gemma_tokenizer,\n",
    "    )\n",
    "    sampled_per_token_models_cross_entropies.append(per_token_models_cross_entropy)\n",
    "    sampled_cross_perplexities.append(cross_perplexities)\n",
    "\n",
    "sampled_per_token_models_cross_entropies = torch.cat(sampled_per_token_models_cross_entropies, dim=0)\n",
    "sampled_cross_perplexities = torch.cat(sampled_cross_perplexities, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Regenerate the ratios\n",
    "ratios = sampled_perplexities / sampled_cross_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    data_to_plot=[mp.item() for mp in sampled_perplexities],\n",
    "    labels=sampled_data[\"generated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    data_to_plot=[mp.item() for mp in sampled_cross_perplexities],\n",
    "    labels=sampled_data[\"generated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(\n",
    "    data_to_plot=[r.item() for r in ratios],\n",
    "    labels=sampled_data[\"generated\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Homoglyphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unicode Normal Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://unicode.org/reports/tr15/images/UAX15-NormFig6.jpg)\n",
    "\n",
    "_(source: unicode.org)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicodedata.is_normalized('NFKD', sentence_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Up till this point, we've analyzed homoglyphs for English characters that we take from other languages..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## But what happens when we consider other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://images.ctfassets.net/rporu91m20dc/7beVMBSmVT2VwbZxsrw9DM/78d0636ce5b21cc1206a9512385fce15/IndianaJones_LargeHero_ReleaseDateReveal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# _Quantifying Character Similarity with Vision Transformers_\n",
    "\n",
    "## Xinmei Yang, Abhishek Arora, Shao-Yu Jheng, Melissa Dell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "HZ_CHAR_1 = \"Ë®Ä\"\n",
    "HZ_CHAR_2 = \"Ë®Å\"\n",
    "\n",
    "display(Markdown(f\"# {HZ_CHAR_1}\"))\n",
    "display(Markdown(f\"# {HZ_CHAR_2}\"))\n",
    "display(HZ_CHAR_1 == HZ_CHAR_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "display(unicodedata.name(HZ_CHAR_1))\n",
    "display(unicodedata.name(HZ_CHAR_2))\n",
    "display(unicodedata.is_normalized('NFKD', HZ_CHAR_1))\n",
    "display(unicodedata.is_normalized('NFKD', HZ_CHAR_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unicodedata.east_asian_width(HZ_CHAR_1))\n",
    "display(unicodedata.east_asian_width(HZ_CHAR_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_...so how can we tell them apart?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "rise": {
   "auto_select": "none",
   "auto_select_fragment": false,
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
